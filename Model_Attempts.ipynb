{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP+Ob8JCn/NTvVhuBUyW0fT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/inesfrsantos/MachineLearning/blob/main/Model_Attempts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple self made CNN"
      ],
      "metadata": {
        "id": "rcN-EWTEPRHM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUof-PlRIN_Y",
        "outputId": "84d0bb18-b155-47e1-9e23-08261ef75d9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 62, 62, 32)        320       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 31, 31, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 29, 29, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 14, 14, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 12544)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               1605760   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 128)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 28)                3612      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,628,188\n",
            "Trainable params: 1,628,188\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "#Load packages\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "# Define the input shape\n",
        "input_shape = (64, 64, 1)\n",
        "\n",
        "# Initialize the model\n",
        "model = Sequential()\n",
        "\n",
        "# Add the first convolutional layer\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
        "\n",
        "# Add the first max pooling layer\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "# Add the second convolutional layer\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "\n",
        "# Add the second max pooling layer\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "# Add the flatten layer\n",
        "model.add(Flatten())\n",
        "\n",
        "# Add the dense layer\n",
        "model.add(Dense(128, activation='relu'))\n",
        "\n",
        "# Add the dropout layer\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Add the output layer\n",
        "model.add(Dense(28, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Complex Sef Made CNN"
      ],
      "metadata": {
        "id": "jVXqO13aPIjz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the first convolutional layer with 32 filters of size (3, 3), using 'same' padding\n",
        "# Input shape is (64, 64, 1) for grayscale images, and ReLU activation function\n",
        "model.add(Conv2D(32, (3, 3), padding='same', input_shape=(64, 64, 1), activation='relu'))\n",
        "\n",
        "# Add batch normalization to normalize the activations of the previous layer\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Add the second convolutional layer with 32 filters of size (3, 3), using 'same' padding\n",
        "# ReLU activation function is used\n",
        "model.add(Conv2D(32, (3, 3), padding='same', activation='relu'))\n",
        "\n",
        "# Add batch normalization to normalize the activations of the previous layer\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Add max pooling layer with a pool size of (2, 2) to downsample the image\n",
        "# Dropout of 0.25 is applied to randomly set 25% of the input units to 0 during training\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# Add the third convolutional layer with 64 filters of size (3, 3), using 'same' padding\n",
        "# ReLU activation function is used\n",
        "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "\n",
        "# Add batch normalization to normalize the activations of the previous layer\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Add the fourth convolutional layer with 64 filters of size (3, 3), using 'same' padding\n",
        "# ReLU activation function is used\n",
        "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "\n",
        "# Add batch normalization to normalize the activations of the previous layer\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Add max pooling layer with a pool size of (2, 2) to downsample the image\n",
        "# Dropout of 0.25 is applied to randomly set 25% of the input units to 0 during training\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# Add the fifth convolutional layer with 128 filters of size (3, 3), using 'same' padding\n",
        "# ReLU activation function is used\n",
        "model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
        "\n",
        "# Add batch normalization to normalize the activations of the previous layer\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Add the sixth convolutional layer with 128 filters of size (3, 3), using 'same' padding\n",
        "# ReLU activation function is used\n",
        "model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
        "\n",
        "# Add batch normalization to normalize the activations of the previous layer\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Add max pooling layer with a pool size of (2, 2) to downsample the image\n",
        "# Dropout of 0.25 is applied to randomly set 25% of the input units to 0 during training\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# Flatten the output from the previous layer to a 1D vector\n",
        "model.add(Flatten())\n",
        "\n",
        "# Add a fully connected dense layer with 512 units and ReLU activation function\n",
        "model.add(Dense(512, activation='relu'))\n",
        "\n",
        "# Add batch normalization to normalize the activations of the previous layer\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Add Dropout o.5, meaning half of the neurons will be dropped\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Add the output layer with 28 units (one for each class) and softmax activation\n",
        "model.add(Dense(28, activation='softmax'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "z-bFog1fKAXx",
        "outputId": "f1ea2b4d-02b6-4682-b7ec-9b4b24e782f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-fa32be22ea45>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Add the first convolutional layer with 32 filters of size (3, 3), using 'same' padding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Input shape is (64, 64, 1) for grayscale images, and ReLU activation function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'same'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Add batch normalization to normalize the activations of the previous layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/trackable/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    251\u001b[0m             \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mndim\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_ndim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    254\u001b[0m                     \u001b[0;34mf'Input {input_index} of layer \"{layer_name}\" '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m                     \u001b[0;34m\"is incompatible with the layer: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input 0 of layer \"conv2d_14\" is incompatible with the layer: expected min_ndim=4, found ndim=2. Full shape received: (None, 28)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The architecture used for Arabic sign language image classification is effective because it includes more layers and parameters, allowing the model to learn more complex features from the images. The use of batch normalization helps to normalize the input, which can improve the convergence of the model during training. The addition of dropout layers helps to prevent overfitting by randomly dropping out nodes during training. The use of multiple convolutional and max pooling layers allows the model to learn hierarchical representations of the input images, capturing both local and global features. This architecture can handle more complex patterns and variations in the input data compared to a simpler model with only a few layers, like the simple CNN."
      ],
      "metadata": {
        "id": "RYixPV--PHfH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GoogLeNet Inspired CNN"
      ],
      "metadata": {
        "id": "pEsjXz6ERwEI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Option 1"
      ],
      "metadata": {
        "id": "Xn85F82zUxu3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dropout, concatenate, Flatten, Dense\n",
        "\n",
        "# Define the input shape\n",
        "input_shape = (64, 64, 1)\n",
        "\n",
        "# Input layer\n",
        "input_layer = Input(shape=input_shape)\n",
        "\n",
        "# First convolutional block\n",
        "conv1_1 = Conv2D(64, (1,1), activation='relu', padding='same')(input_layer)\n",
        "conv1_2 = Conv2D(64, (3,3), activation='relu', padding='same')(conv1_1)\n",
        "conv1_3 = Conv2D(192, (3,3), activation='relu', padding='same')(conv1_2)\n",
        "pool1 = MaxPooling2D((3,3), strides=(2,2), padding='same')(conv1_3)\n",
        "\n",
        "# Second convolutional block\n",
        "conv2_1 = Conv2D(64, (1,1), activation='relu', padding='same')(pool1)\n",
        "conv2_2 = Conv2D(128, (3,3), activation='relu', padding='same')(conv2_1)\n",
        "conv2_3 = Conv2D(256, (3,3), activation='relu', padding='same')(conv2_2)\n",
        "pool2 = MaxPooling2D((3,3), strides=(2,2), padding='same')(conv2_3)\n",
        "\n",
        "# Third convolutional block with additional dropout and batch normalization\n",
        "conv3_1 = Conv2D(128, (1,1), activation='relu', padding='same')(pool2)\n",
        "conv3_2 = Conv2D(256, (3,3), activation='relu', padding='same')(conv3_1)\n",
        "conv3_3 = Conv2D(512, (3,3), activation='relu', padding='same')(conv3_2)\n",
        "pool3 = MaxPooling2D((3,3), strides=(2,2), padding='same')(conv3_3)\n",
        "dropout1 = Dropout(0.4)(pool3)\n",
        "batch_norm1 = BatchNormalization()(dropout1)\n",
        "\n",
        "# Fourth convolutional block with additional dropout and batch normalization\n",
        "conv4_1 = Conv2D(256, (1,1), activation='relu', padding='same')(batch_norm1)\n",
        "conv4_2 = Conv2D(512, (3,3), activation='relu', padding='same')(conv4_1)\n",
        "conv4_3 = Conv2D(1024, (3,3), activation='relu', padding='same')(conv4_2)\n",
        "dropout2 = Dropout(0.4)(conv4_3)\n",
        "batch_norm2 = BatchNormalization()(dropout2)\n",
        "\n",
        "# Fifth convolutional block with additional dropout and batch normalization\n",
        "conv5_1 = Conv2D(256, (1,1), activation='relu', padding='same')(batch_norm2)\n",
        "conv5_2 = Conv2D(512, (3,3), activation='relu', padding='same')(conv5_1)\n",
        "conv5_3 = Conv2D(1024, (3,3), activation='relu', padding='same')(conv5_2)\n",
        "dropout3 = Dropout(0.4)(conv5_3)\n",
        "batch_norm3 = BatchNormalization()(dropout3)\n",
        "\n",
        "# Sixth convolutional block with additional dropout and batch normalization\n",
        "conv6_1 = Conv2D(512, (1,1), activation='relu', padding='same')(concat)\n",
        "conv6_2 = Conv2D(1024, (3,3), activation='relu', padding='same')(conv6_1)\n",
        "dropout4 = Dropout(0.4)(conv6_2)\n",
        "batch_norm4 = BatchNormalization()(dropout4)\n",
        "\n",
        "# Seventh convolutional block with additional dropout and batch normalization\n",
        "conv7_1 = Conv2D(256, (1,1), activation='relu', padding='same')(batch_norm4)\n",
        "conv7_2 = Conv2D(512, (3,3), activation='relu', padding='same')(conv7_1)\n",
        "conv7_3 = Conv2D(1024, (3,3), activation='relu', padding='same')(conv7_2)\n",
        "dropout5 = Dropout(0.4)(conv7_3)\n",
        "batch_norm5 = BatchNormalization()(dropout5)\n",
        "\n",
        "# Eighth convolutional block with additional dropout and batch normalization\n",
        "conv8_1 = Conv2D(256, (1,1), activation='relu', padding='same')(batch_norm5)\n",
        "conv8_2 = Conv2D(512, (3,3), strides=(2,2), activation='relu', padding='same')(conv8_1)\n",
        "conv8_3 = Conv2D(1024, (3,3), activation='relu', padding='same')(conv8_2)\n",
        "dropout6 = Dropout(0.4)(conv8_3)\n",
        "batch_norm6 = BatchNormalization()(dropout6)\n",
        "\n",
        "# Ninth convolutional block with additional dropout and batch normalization\n",
        "conv9_1 = Conv2D(512, (1,1), activation='relu', padding='same')(batch_norm6)\n",
        "conv9_2 = Conv2D(1024, (3,3), activation='relu', padding='same')(conv9_1)\n",
        "dropout7 = Dropout(0.4)(conv9_2)\n",
        "batch_norm7 = BatchNormalization()(dropout7)\n",
        "\n",
        "# Flatten the output of the ninth convolutional block\n",
        "flatten = Flatten()(batch_norm7)\n",
        "\n",
        "# Add two fully connected (dense) layers with additional dropout and batch normalization\n",
        "dense1 = Dense(1024, activation='relu')(flatten)\n",
        "dropout8 = Dropout(0.4)(dense1)\n",
        "batch_norm8 = BatchNormalization()(dropout8)\n",
        "dense2 = Dense(10, activation='softmax')(batch_norm8) # output layer with 10 classes\n",
        "\n",
        "# Define the model with the input layer and the output layer\n",
        "model = Model(inputs=input_layer, outputs=dense2)\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "qAeguEpVRLh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The modifications made to the original GoogLeNet architecture are:\n",
        "\n",
        "Using a grayscale input image (so only 1 channel instead of 3)\n",
        "Changing the input image size to 64x64\n",
        "Adding dropout and batch normalization layers after each of the third, fourth, fifth, sixth, seventh, eighth, and ninth convolutional blocks\n",
        "Changing the number of output classes in the last dense layer to 10 (since we don't know what the specific classification task is, but it's common to have 10 classes for image classification problems)"
      ],
      "metadata": {
        "id": "uo4IbX3dVjNB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Option 2"
      ],
      "metadata": {
        "id": "fCFEAdJlUvzI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, Conv2D, MaxPooling2D, concatenate, AveragePooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "# Define input shape\n",
        "input_shape = (64, 64, 1)\n",
        "\n",
        "# Define input layer\n",
        "inputs = Input(shape=input_shape)\n",
        "\n",
        "# Define GoogLeNet-like architecture\n",
        "# First convolutional layer\n",
        "conv1_7x7_s2 = Conv2D(64, (7, 7), strides=(2, 2), padding='same', activation='relu')(inputs)\n",
        "\n",
        "# Max pooling layer\n",
        "pool1_3x3_s2 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(conv1_7x7_s2)\n",
        "\n",
        "# Second convolutional layer\n",
        "conv2_3x3_reduce = Conv2D(64, (1, 1), padding='same', activation='relu')(pool1_3x3_s2)\n",
        "conv2_3x3 = Conv2D(192, (3, 3), padding='same', activation='relu')(conv2_3x3_reduce)\n",
        "\n",
        "# Max pooling layer\n",
        "pool2_3x3_s2 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(conv2_3x3)\n",
        "\n",
        "# Inception module 1\n",
        "inception_3a_1x1 = Conv2D(64, (1, 1), padding='same', activation='relu')(pool2_3x3_s2)\n",
        "\n",
        "# These 3 convolutional layers are added in the modified GoogLeNet architecture.\n",
        "conv3_3x3 = Conv2D(64, (3, 3), padding='same', activation='relu')(pool2_3x3_s2)\n",
        "conv4_5x5 = Conv2D(32, (5, 5), padding='same', activation='relu')(pool2_3x3_s2)\n",
        "conv5_1x1 = Conv2D(32, (1, 1), padding='same', activation='relu')(pool2_3x3_s2)\n",
        "\n",
        "# Concatenate the output of the convolutional layers above\n",
        "concatenate_1 = concatenate([inception_3a_1x1, conv3_3x3, conv4_5x5, conv5_1x1], axis=3)\n",
        "\n",
        "# Inception module 2\n",
        "inception_3b_1x1 = Conv2D(128, (1, 1), padding='same', activation='relu')(concatenate_1)\n",
        "\n",
        "# These 3 convolutional layers are added in the modified GoogLeNet architecture.\n",
        "conv6_3x3 = Conv2D(128, (3, 3), padding='same', activation='relu')(concatenate_1)\n",
        "conv7_5x5 = Conv2D(64, (5, 5), padding='same', activation='relu')(concatenate_1)\n",
        "conv8_1x1 = Conv2D(64, (1, 1), padding='same', activation='relu')(concatenate_1)\n",
        "\n",
        "# Concatenate the output of the convolutional layers above\n",
        "concatenate_2 = concatenate([inception_3b_1x1, conv6_3x3, conv7_5x5, conv8_1x1], axis=3)\n",
        "\n",
        "# Add a fully connected layer with 1024 neurons\n",
        "model.add(layers.Dense(1024, activation='relu'))\n",
        "\n",
        "# Add a dropout layer to prevent overfitting\n",
        "model.add(layers.Dropout(0.4))\n",
        "\n",
        "# Add the final output layer with 10 neurons (one for each class)\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "l4KwavxQUu7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The proposed architecture is inspired by GoogLeNet, which is a well-known deep learning architecture used for image classification tasks. In the case of Arabic sign language classification, the proposed model takes grayscale images of size 64x64 pixels as input and passes them through a series of convolutional and pooling layers to extract features at different scales. The added layers in this model help improve its accuracy and enhance the ability of the model to capture complex patterns in the input images.\n",
        "\n",
        "In particular, the inception modules in the proposed architecture are designed to capture both local and global features of the input images, which is essential for recognizing signs in Arabic sign language. Additionally, the added dropout layer helps prevent overfitting by randomly dropping out some of the neurons during training, which improves the model's generalization ability.\n",
        "\n",
        "Overall, the proposed architecture is well-suited for the classification task of Arabic sign language because it can effectively capture the intricate hand gestures and motions that are involved in this language.\n"
      ],
      "metadata": {
        "id": "uaBLlooMVeeu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4jZ5q-veU7u2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}